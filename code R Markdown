---
title: "NLP Project: Investigating the environmental discourse from 1970 to 2018 with Quanteda"
author: "Jens Anderer"
output: html_notebook
---

```{r message=FALSE, include=FALSE}
#Loading packages
library(readtext)
library(quanteda)
library(dplyr)
library(tidyr)
library(tidyverse)
library(stringr)
library(ggplot2)
library(rworldmap)
library(RColorBrewer)
library(haven)
library(readxl)
library(rworldmap)
```

#Downloading and Loading the Data

You have to download the UNGD Corpus, which contains speeches from head of states and leading diplomats, 
held between 1970 and 2018 at the UN General Debate. 

You can find further information on the dataset via the following link (https://github.com/sjankin/UnitedNations) 
and you can download the zip folder called "UNGDC 1970-2018.zip" here: 
https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y

After you have downloaded and unpacked the zip folder, you will end up with a folder called "Converted sessions", 
containing all speeches of the UN General Debate ordered by year and country.  
If you save this folder on your desktop, you only need to change the "UserName" within the first line of code below.
You can then run the rest of the code automatically.

If you have any questions, please contact me at: j.anderer@mia.hertie-school.org

```{r include=FALSE}
#Loading data
DATA_DIR <- "/Users/UserName/Desktop/"

ungd_files <- readtext(paste0(DATA_DIR, "Converted sessions/*"), 
                                 docvarsfrom = "filenames", 
                                 dvsep="_", 
                                 docvarnames = c("Country", "Session", "Year"))


ungd_files$doc_id <- str_replace(ungd_files$doc_id , ".txt", "") %>%
   str_replace(. , "_\\d{2}", "")
```

#Research Question

In the light of recent school demonstrations against climate change or in favor of climate action, 
I want to investigate the general environmental debate or discourse since 1970 and until today. 
For many researchers, the moon landing in 1969 was the starting point for the emergence of environmental consciousness. 
This is true due to the fact, that mankind obtained the first picture of the Earth from above, 
which was triggering our environmental understanding. In this sense, and from 1970 onwards, 
I expect to see an increasing trajectory where the environmental debate becomes more and more important with time, peaking nowadays. 

```{r include=FALSE}
#creating corpus object of all years between 1970 and 2018
ungd_corpus <- corpus(ungd_files, text_field = "text") 
summary(ungd_corpus)

#setting to character
ungd_corpus1 <- as.character(ungd_corpus)
class(ungd_corpus1)
```

The term "environment" appears 20.152 times within the whole corpus of text files. 

```{r echo=FALSE}
length(unlist(strsplit(ungd_corpus1, "environment"))) 
```

#Pre-processing and Tokenizing 

In the following, I pre-process and tokenize the data.

```{r include=FALSE}
#Tokenization and basic pre-processing
tok <- tokens(ungd_corpus, what = "word",
              remove_punct = TRUE,
              remove_symbols = TRUE,
              remove_numbers = TRUE,
              remove_twitter = TRUE,
              remove_url = TRUE,
              remove_hyphens = TRUE,
              verbose = TRUE, 
              include_docvars = TRUE)

tok.r <- tokens_tolower(tok)

tok.s <- tokens_select(tok.r, stopwords("english"), selection = "remove", padding = FALSE)

tok.n <- tokens(tok.s, ngrams = c(1:2), include_docvars = TRUE) 

# not rely on the defaults exclusively, padding, sometimes needs to be changed to TRUE 
# specification that we want unigrams and bigrams 
```

I can also create a dfm, which takes all functions of tokenizing, normalizing, and stemming. 
Besides to stopwords, I removed numbers, punctuations, symbols and other unnecessary information. 

```{r include=FALSE}
dfm <- dfm(ungd_corpus, what = c("word"), remove = stopwords("english"), stem = TRUE, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE, remove_twitter = TRUE, remove_hyphens = TRUE, remove_url = TRUE)
```

The same - without stemming - can be done here with the `tokens()` function.

```{r include=FALSE}
data_corpus_ungd_withoutstemming <- tokens(ungd_corpus, what = c("word"), remove = stopwords("english"), remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE, remove_twitter = TRUE, remove_hyphens = TRUE, remove_url = TRUE)
```

#Applying environmental dictionary

In the following, I introduce a dictionary of several important words with regard to the general environmental debate.

```{r include=FALSE}
#creating a dictionary
my_list_environment <- list("environment", "climate_change", "sustainability", "sustainable_development", "global_warming", "acid_rain", "green_house", "temperature", "extreme_weather", "global_environmental_change", "climate_variability", "greenhouse", "carbon", "renewable_energy", "carbon_emission", "co2_emission", "climate_pollutant", "climate_vulnerability", "natural_desaster", "degrowth", "green_growth", "Brundtland")

tok.compound <- tokens_compound(tok.n, my_list_environment, valuetype = "fixed", concatenator = "_")

environment_dict <- dictionary(list(environment = c("environment", "climate_change", "sustainability", "sustainable_development", "global_warming", "acid_rain", "green_house", "temperature", "extreme_weather", "global_environmental_change", "climate_variability", "greenhouse", "carbon", "renewable_energy", "carbon_emission", "co2_emission", "climate_pollutant", "climate_vulnerability", "natural_desaster", "degrowth", "green_growth", "Brundtland")))
```
 
## Look up environmental term counts

```{r include=FALSE}
# create a dfm, document-feature-matrix 

total_dfm <- dfm(tok.compound)

environment_dfm <- dfm_lookup(total_dfm, environment_dict, valuetype = "fixed")

#creating a data frame 
environment <- convert(environment_dfm, to = "data.frame")

#converting documentID into two columns - country and year
counts <- environment %>%
  separate(document, c("country", "year"), "_")

#year as numeric
counts$year <- as.numeric(counts$year)
```

I perform a key-words-in-context search for environmental key terms. 
The window is set to 25 words before and after the term - reflecting approximately half a paragraph before 
and after the term (on average a paragraph in English is 50 words). 

Below, I printed two summaries, the first with the valuetype `fixed`, the second with `regex`, which allows for more flexibility. 
Hence, the patterns appear more often when using `regex`. 
I continue the analysis with the `fixed` example (where more word cleaning is applied). 

```{r include=FALSE}
tok.environment <- kwic(tok.compound, c("environment", "climate_change", "sustainability", "sustainable_development", "global_warming", "acid_rain", "green_house", "temperature", "extreme_weather", "global_environmental_change", "climate_variability", "greenhouse", "carbon", "renewable_energy", "carbon_emission", "co2_emission", "climate_pollutant", "climate_vulnerability", "natural_desaster", "degrowth", "green_growth", "Brundtland"), window = 25, valuetype = "fixed")

tok.environment2 <- kwic(tok.compound, c("environment", "climate_change", "sustainability", "sustainable_development", "global_warming", "acid_rain", "green_house", "temperature", "extreme_weather", "global_environmental_change", "climate_variability", "greenhouse", "carbon", "renewable_energy", "carbon_emission", "co2_emission", "climate_pollutant", "climate_vulnerability", "natural_desaster", "degrowth", "green_growth", "Brundtland"), window = 25, valuetype = "regex")

summary(tok.environment)
summary(tok.environment2)
#readr::write_csv(tok.environment, "environment_kwic_25_fixed.csv")
```

We can see that "environment" is the most common pattern within the dictionary. 
Two other important wordings are both "sustainable development" and "climate change". 

Text surrounding environmental terms (half a paragraph context) are saved and transformed to a corpus object and 
tokenized for second-stage analysis. Tokens are then converted to a document-feature matrix (DFM) for analysis.

```{r include=FALSE}
corpus_environment <- corpus(tok.environment, split_context = FALSE)
tok.environment <- tokens(corpus_environment, verbose = TRUE)
environment_tok_dfm <- dfm(tok.environment)

#document names to reflect ISO code and Year. KWIC transformation above replaced docids.
docvars(environment_tok_dfm, "docid") <- str_sub(environment_tok_dfm@Dimnames$docs, start=1, end=8)

#grouping DFM by country-year, so that there's one row per country-year
environment_dfm <- dfm_group(environment_tok_dfm, groups = "docid")
```

## Time series of total counts plot

Here, I create a time series analysis of the total counts plot of words related to the environmental debate. 
As we can see, the environmental debate indeed took off after the moon landing and became more and more important. 
The environmental debate was further triggered around 1987 when the Brundtland report was published and the concept of 
sustainable development became popular. 

Between 2005 and 2010, there was another momentum when environmental issues became popular on the international stage. 
This might reflect continuing international debates as well as increasing environmental consciousness. 
Another factor could be of economic nature, namely, the appearance of renewable energy as a new industry. 
In general, the importance of environmental issues within UN General Debates is continuously increasing and reached another peak in 2016. 

```{r echo=FALSE}
# calculating the total number of mentions by year
sum <- summarise(group_by(counts, year), count = sum(environment), mean = mean(environment))

sum_counts <- summarise(group_by(counts, year), 
                 count_CC = sum(environment),  
                 mean_CC = mean(environment))

sum_overall <- merge(sum_counts, sum, by = "year")

ggplot(sum_overall, aes(x=year)) +
  theme_bw() +
  geom_line(aes(y = count_CC), colour = "green", alpha = 0.9) +#  geom_line(aes(y= count), colour = "black", alpha = 0.9) +
  #ggtitle("Political engagement with the intersection of climate change and health") + 
  ylab("Total number of references per UNGD session") + xlab("Year") + 
#  scale_y_continuous(limits=c(0, 140), breaks = c(1, 50, 100, 134)) +
  scale_x_continuous(limits=c(1970, 2017), breaks = c(1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015)) +
 annotate("text", x = 2012, y = 300, label = "Environment", colour = "black")

ggsave("timeseries_environmental_discourse_1970-2017.pdf")
```

Below, the same development is illustrated, from 2000 until 2017 only. 

```{r echo=FALSE}
ggplot(sum_overall, aes(x=year)) +
  theme_bw() +
  geom_line(aes(y = count_CC), colour = "green", alpha = 0.9) +#  geom_line(aes(y= count), colour = "black", alpha = 0.9) +
  #ggtitle("Political engagement with the intersection of climate change and health") + 
  ylab("Total number of references per UNGD session") + xlab("Year") + 
#  scale_y_continuous(limits=c(0, 140), breaks = c(1, 50, 100, 134)) +
  scale_x_continuous(limits=c(2000, 2017), breaks = c(2000, 2005, 2010, 2015)) +
 annotate("text", x = 2012, y = 300, label = "Environment", colour = "black")

ggsave("timeseries_environmental_discourse_2000-2017.pdf")
```
